{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network from Scratch\n",
    "\n",
    "Simple program wich implements a Neural Network with customizable number of input features and output classes.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "nn = NeuralNetwork(n_input_features = 3, output_tpl = (1, sigmoid), hidden_layers = [(2, sigmoid)])\n",
    "\n",
    "training_inputs = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_outputs = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "nn.train(training_inputs, training_outputs, epochs = 20000, learning_rate = 0.1)\n",
    "```\n",
    "\n",
    "Main variables involved:\n",
    "\n",
    "* $m$ number of samples used for the training process\n",
    "* $L$ total number of layers (hidden + input values + output layer)\n",
    "\n",
    "In every layer we then have the following variables:\n",
    "\n",
    "* $n$ input features (i.e. number of neurons in layer 0)\n",
    "* $N$ number of neurons\n",
    "\n",
    "\n",
    "## Training \n",
    "\n",
    "The training is made of two main processes, which are repeated a number of times equal to `epochs` in the example before.\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "\n",
    "### Backward Propagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return(1 / (1 + np.exp(-z)))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return(sigmoid(z)*(1-sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_value(y_best, y_true):\n",
    "    cost = - np.mean(np.dot(np.log(y_best).T, y_true) + np.dot(np.log(1 - y_best).T, 1 - y_true))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    '''\n",
    "    Every input (x) is a matrix (m x n) with m->n_samples and n->n_features \n",
    "    Given N->n_neurons, the weight matrix (W) is defined as (n x N), so that:\n",
    "    dim(x dot W) = dim(z)\n",
    "    (m x n) x (n x N) = (m x N)\n",
    "    '''\n",
    "    def __init__(self, *args):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.activation_function = None\n",
    "        if args:\n",
    "            n_features, n_neurons, activation_function = args\n",
    "            self.W = np.random.rand(n_features, n_neurons)\n",
    "            self.b = np.zeros((1, n_neurons))\n",
    "            self.activation_function = activation_function\n",
    "            \n",
    "    def forward(self, A_prev):\n",
    "        self.Z = np.dot(A_prev, self.W) + self.b\n",
    "        self.A = self.activation_function(self.Z)\n",
    "        \n",
    "    def backward(self, delta, Z_prev, A_prev):\n",
    "        self.delta = delta\n",
    "        self.nabla_W = np.dot(A_prev.T, self.delta)\n",
    "        self.nabla_b = np.sum(self.delta)\n",
    "        self.delta_prev = np.dot(self.delta, self.W.T) * sigmoid_gradient(Z_prev)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.W -= learning_rate * self.nabla_W\n",
    "        self.b -= learning_rate * self.nabla_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_input_features, output_tpl, hidden_layers = None):\n",
    "        np.random.seed(0)\n",
    "        if hidden_layers is None:\n",
    "            hidden_layers = []\n",
    "        layers_tpl = hidden_layers + [output_tpl]\n",
    "        self.layers = []\n",
    "        layers_inputs = [n_input_features] + [tpl[0] for tpl in layers_tpl[:-1]]\n",
    "        for n_features, n_neurons, activation_function in zip(layers_inputs, \n",
    "                                                             [tpl[0] for tpl in layers_tpl], \n",
    "                                                             [tpl[1] for tpl in layers_tpl]):\n",
    "            self.layers.append(Layer(n_features, n_neurons, activation_function))\n",
    "        self.layers.insert(0, Layer()) # Layer 0, it has only self.a\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.layers[0].A = inputs\n",
    "        self.layers[0].Z = inputs\n",
    "        for index, layer in enumerate(self.layers[1:]):\n",
    "            layer.forward(self.layers[index].A)\n",
    "        self.y_best = self.layers[-1].A\n",
    "            \n",
    "    def backward(self, y_true, learning_rate):\n",
    "        delta = (self.layers[-1].A - y_true) * sigmoid_gradient(self.layers[-1].Z) \n",
    "        Z_prev = self.layers[-2].Z\n",
    "        A_prev = self.layers[-2].A\n",
    "        for index, layer in reversed(list(enumerate(self.layers[1:]))):\n",
    "            layer.backward(delta, Z_prev, A_prev)\n",
    "            delta = layer.delta_prev\n",
    "            Z_prev = self.layers[index - 1].Z\n",
    "            A_prev = self.layers[index - 1].A\n",
    "            layer.update(learning_rate)\n",
    "        \n",
    "    def train(self, inputs, y_true, epochs, learning_rate):\n",
    "        self.cost_history = []\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(inputs)\n",
    "            cost = get_cost_value(self.y_best, y_true)\n",
    "            self.cost_history.append(cost)\n",
    "            self.backward(y_true, learning_rate)\n",
    "            if epoch%100 == 0:\n",
    "                print('Epoch: {}, Loss: {}'.format(epoch, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "#nn = NeuralNetwork(n_input_features = 3, output_tpl = (1, sigmoid), hidden_layers = [(2, sigmoid), (2, sigmoid)])\n",
    "nn = NeuralNetwork(n_input_features = 3, output_tpl = (1, sigmoid), hidden_layers = [(2, sigmoid)])\n",
    "#nn = NeuralNetwork(n_input_features = 3, output_tpl = (1, sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_outputs = np.array([[0, 1, 1, 0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.1069754330760464\n",
      "Epoch: 100, Loss: 2.591495173097795\n",
      "Epoch: 200, Loss: 2.3705301395693468\n",
      "Epoch: 300, Loss: 1.975173336162457\n",
      "Epoch: 400, Loss: 1.5267753534898993\n",
      "Epoch: 500, Loss: 1.170459612462161\n",
      "Epoch: 600, Loss: 0.9282097506472109\n",
      "Epoch: 700, Loss: 0.7663134837899209\n",
      "Epoch: 800, Loss: 0.6543249120286788\n",
      "Epoch: 900, Loss: 0.5733931951701743\n",
      "Epoch: 1000, Loss: 0.5125131995199665\n",
      "Epoch: 1100, Loss: 0.46514028210438224\n",
      "Epoch: 1200, Loss: 0.4272315131837394\n",
      "Epoch: 1300, Loss: 0.3961864484405987\n",
      "Epoch: 1400, Loss: 0.3702684569693821\n",
      "Epoch: 1500, Loss: 0.34827835164300447\n",
      "Epoch: 1600, Loss: 0.3293634165802995\n",
      "Epoch: 1700, Loss: 0.31290155255798835\n",
      "Epoch: 1800, Loss: 0.2984285973706833\n",
      "Epoch: 1900, Loss: 0.285591322012677\n",
      "Epoch: 2000, Loss: 0.2741161872385674\n",
      "Epoch: 2100, Loss: 0.2637880582599279\n",
      "Epoch: 2200, Loss: 0.25443537904142066\n",
      "Epoch: 2300, Loss: 0.2459196378380391\n",
      "Epoch: 2400, Loss: 0.23812774578371895\n",
      "Epoch: 2500, Loss: 0.23096643219122576\n",
      "Epoch: 2600, Loss: 0.2243580612283689\n",
      "Epoch: 2700, Loss: 0.21823746688235068\n",
      "Epoch: 2800, Loss: 0.21254952843550276\n",
      "Epoch: 2900, Loss: 0.20724729190339994\n",
      "Epoch: 3000, Loss: 0.20229049912765484\n",
      "Epoch: 3100, Loss: 0.19764442483330674\n",
      "Epoch: 3200, Loss: 0.19327894887239355\n",
      "Epoch: 3300, Loss: 0.18916780988889179\n",
      "Epoch: 3400, Loss: 0.18528800024620157\n",
      "Epoch: 3500, Loss: 0.18161927191144855\n",
      "Epoch: 3600, Loss: 0.17814373020587895\n",
      "Epoch: 3700, Loss: 0.17484549766917368\n",
      "Epoch: 3800, Loss: 0.1717104342742945\n",
      "Epoch: 3900, Loss: 0.16872590323722697\n",
      "Epoch: 4000, Loss: 0.16588057395353292\n",
      "Epoch: 4100, Loss: 0.16316425534771542\n",
      "Epoch: 4200, Loss: 0.1605677542766727\n",
      "Epoch: 4300, Loss: 0.15808275468331656\n",
      "Epoch: 4400, Loss: 0.15570171402303318\n",
      "Epoch: 4500, Loss: 0.1534177741376134\n",
      "Epoch: 4600, Loss: 0.15122468426871194\n",
      "Epoch: 4700, Loss: 0.14911673431592842\n",
      "Epoch: 4800, Loss: 0.14708869677619163\n",
      "Epoch: 4900, Loss: 0.14513577606873448\n",
      "Epoch: 5000, Loss: 0.14325356416697915\n",
      "Epoch: 5100, Loss: 0.1414380016355825\n",
      "Epoch: 5200, Loss: 0.13968534331576118\n",
      "Epoch: 5300, Loss: 0.1379921280211349\n",
      "Epoch: 5400, Loss: 0.13635515170477824\n",
      "Epoch: 5500, Loss: 0.13477144363973803\n",
      "Epoch: 5600, Loss: 0.13323824522325534\n",
      "Epoch: 5700, Loss: 0.13175299107168872\n",
      "Epoch: 5800, Loss: 0.13031329212076387\n",
      "Epoch: 5900, Loss: 0.12891692048584782\n",
      "Epoch: 6000, Loss: 0.12756179587077182\n",
      "Epoch: 6100, Loss: 0.12624597334240764\n",
      "Epoch: 6200, Loss: 0.12496763231253138\n",
      "Epoch: 6300, Loss: 0.12372506658928346\n",
      "Epoch: 6400, Loss: 0.12251667537824945\n",
      "Epoch: 6500, Loss: 0.12134095512840012\n",
      "Epoch: 6600, Loss: 0.1201964921311822\n",
      "Epoch: 6700, Loss: 0.11908195579230996\n",
      "Epoch: 6800, Loss: 0.11799609250550783\n",
      "Epoch: 6900, Loss: 0.11693772006589628\n",
      "Epoch: 7000, Loss: 0.11590572256798297\n",
      "Epoch: 7100, Loss: 0.11489904573958824\n",
      "Epoch: 7200, Loss: 0.11391669266856602\n",
      "Epoch: 7300, Loss: 0.11295771988400655\n",
      "Epoch: 7400, Loss: 0.11202123375785192\n",
      "Epoch: 7500, Loss: 0.11110638719655044\n",
      "Epoch: 7600, Loss: 0.11021237659564906\n",
      "Epoch: 7700, Loss: 0.10933843903309165\n",
      "Epoch: 7800, Loss: 0.10848384967951227\n",
      "Epoch: 7900, Loss: 0.10764791940606715\n",
      "Epoch: 8000, Loss: 0.10682999257231561\n",
      "Epoch: 8100, Loss: 0.10602944497841624\n",
      "Epoch: 8200, Loss: 0.10524568196748109\n",
      "Epoch: 8300, Loss: 0.104478136665284\n",
      "Epoch: 8400, Loss: 0.10372626834579421\n",
      "Epoch: 8500, Loss: 0.10298956091207495\n",
      "Epoch: 8600, Loss: 0.10226752148309304\n",
      "Epoch: 8700, Loss: 0.10155967907785285\n",
      "Epoch: 8800, Loss: 0.10086558338907112\n",
      "Epoch: 8900, Loss: 0.10018480363930096\n",
      "Epoch: 9000, Loss: 0.09951692751306977\n",
      "Epoch: 9100, Loss: 0.09886156015914767\n",
      "Epoch: 9200, Loss: 0.09821832325759436\n",
      "Epoch: 9300, Loss: 0.09758685414669035\n",
      "Epoch: 9400, Loss: 0.09696680500528307\n",
      "Epoch: 9500, Loss: 0.09635784208645058\n",
      "Epoch: 9600, Loss: 0.09575964499873713\n",
      "Epoch: 9700, Loss: 0.09517190603152378\n",
      "Epoch: 9800, Loss: 0.09459432952137695\n",
      "Epoch: 9900, Loss: 0.09402663125647825\n",
      "Epoch: 10000, Loss: 0.09346853791646964\n",
      "Epoch: 10100, Loss: 0.09291978654526503\n",
      "Epoch: 10200, Loss: 0.09238012405455855\n",
      "Epoch: 10300, Loss: 0.09184930675596237\n",
      "Epoch: 10400, Loss: 0.09132709991983404\n",
      "Epoch: 10500, Loss: 0.09081327735903316\n",
      "Epoch: 10600, Loss: 0.0903076210359571\n",
      "Epoch: 10700, Loss: 0.08980992069134183\n",
      "Epoch: 10800, Loss: 0.08931997349342256\n",
      "Epoch: 10900, Loss: 0.08883758370615052\n",
      "Epoch: 11000, Loss: 0.08836256237526133\n",
      "Epoch: 11100, Loss: 0.08789472703107287\n",
      "Epoch: 11200, Loss: 0.08743390140697514\n",
      "Epoch: 11300, Loss: 0.08697991517264383\n",
      "Epoch: 11400, Loss: 0.0865326036810811\n",
      "Epoch: 11500, Loss: 0.08609180772864336\n",
      "Epoch: 11600, Loss: 0.08565737332728707\n",
      "Epoch: 11700, Loss: 0.0852291514882965\n",
      "Epoch: 11800, Loss: 0.08480699801682191\n",
      "Epoch: 11900, Loss: 0.08439077331660338\n",
      "Epoch: 12000, Loss: 0.08398034220427955\n",
      "Epoch: 12100, Loss: 0.08357557373274208\n",
      "Epoch: 12200, Loss: 0.08317634102301762\n",
      "Epoch: 12300, Loss: 0.08278252110419271\n",
      "Epoch: 12400, Loss: 0.08239399476094213\n",
      "Epoch: 12500, Loss: 0.0820106463882285\n",
      "Epoch: 12600, Loss: 0.08163236385278586\n",
      "Epoch: 12700, Loss: 0.0812590383610162\n",
      "Epoch: 12800, Loss: 0.08089056433294861\n",
      "Epoch: 12900, Loss: 0.0805268392819411\n",
      "Epoch: 13000, Loss: 0.08016776369981701\n",
      "Epoch: 13100, Loss: 0.07981324094714845\n",
      "Epoch: 13200, Loss: 0.07946317714841793\n",
      "Epoch: 13300, Loss: 0.07911748109180615\n",
      "Epoch: 13400, Loss: 0.07877606413336481\n",
      "Epoch: 13500, Loss: 0.0784388401053542\n",
      "Epoch: 13600, Loss: 0.07810572522852735\n",
      "Epoch: 13700, Loss: 0.0777766380281704\n",
      "Epoch: 13800, Loss: 0.07745149925370383\n",
      "Epoch: 13900, Loss: 0.07713023180167433\n",
      "Epoch: 14000, Loss: 0.07681276064196288\n",
      "Epoch: 14100, Loss: 0.07649901274706206\n",
      "Epoch: 14200, Loss: 0.07618891702426023\n",
      "Epoch: 14300, Loss: 0.075882404250606\n",
      "Epoch: 14400, Loss: 0.07557940701051069\n",
      "Epoch: 14500, Loss: 0.07527985963586875\n",
      "Epoch: 14600, Loss: 0.07498369814857217\n",
      "Epoch: 14700, Loss: 0.07469086020531368\n",
      "Epoch: 14800, Loss: 0.07440128504456633\n",
      "Epoch: 14900, Loss: 0.07411491343564143\n",
      "Epoch: 15000, Loss: 0.0738316876297301\n",
      "Epoch: 15100, Loss: 0.07355155131283653\n",
      "Epoch: 15200, Loss: 0.07327444956051976\n",
      "Epoch: 15300, Loss: 0.07300032879435892\n",
      "Epoch: 15400, Loss: 0.07272913674006996\n",
      "Epoch: 15500, Loss: 0.07246082238719437\n",
      "Epoch: 15600, Loss: 0.0721953359502994\n",
      "Epoch: 15700, Loss: 0.07193262883161418\n",
      "Epoch: 15800, Loss: 0.07167265358504593\n",
      "Epoch: 15900, Loss: 0.07141536388151493\n",
      "Epoch: 16000, Loss: 0.07116071447554884\n",
      "Epoch: 16100, Loss: 0.07090866117308936\n",
      "Epoch: 16200, Loss: 0.07065916080045159\n",
      "Epoch: 16300, Loss: 0.07041217117439488\n",
      "Epoch: 16400, Loss: 0.07016765107325242\n",
      "Epoch: 16500, Loss: 0.06992556020908326\n",
      "Epoch: 16600, Loss: 0.06968585920079491\n",
      "Epoch: 16700, Loss: 0.0694485095482067\n",
      "Epoch: 16800, Loss: 0.06921347360700594\n",
      "Epoch: 16900, Loss: 0.06898071456456978\n",
      "Epoch: 17000, Loss: 0.06875019641661155\n",
      "Epoch: 17100, Loss: 0.06852188394461993\n",
      "Epoch: 17200, Loss: 0.0682957426940612\n",
      "Epoch: 17300, Loss: 0.06807173895331274\n",
      "Epoch: 17400, Loss: 0.06784983973330039\n",
      "Epoch: 17500, Loss: 0.06763001274781015\n",
      "Epoch: 17600, Loss: 0.06741222639444971\n",
      "Epoch: 17700, Loss: 0.06719644973623481\n",
      "Epoch: 17800, Loss: 0.06698265248377386\n",
      "Epoch: 17900, Loss: 0.06677080497803227\n",
      "Epoch: 18000, Loss: 0.066560878173651\n",
      "Epoch: 18100, Loss: 0.06635284362279958\n",
      "Epoch: 18200, Loss: 0.0661466734595422\n",
      "Epoch: 18300, Loss: 0.06594234038470148\n",
      "Epoch: 18400, Loss: 0.06573981765119921\n",
      "Epoch: 18500, Loss: 0.06553907904985119\n",
      "Epoch: 18600, Loss: 0.065340098895612\n",
      "Epoch: 18700, Loss: 0.06514285201423838\n",
      "Epoch: 18800, Loss: 0.06494731372937003\n",
      "Epoch: 18900, Loss: 0.064753459850002\n",
      "Epoch: 19000, Loss: 0.06456126665833889\n",
      "Epoch: 19100, Loss: 0.06437071089802013\n",
      "Epoch: 19200, Loss: 0.06418176976269767\n",
      "Epoch: 19300, Loss: 0.06399442088495619\n",
      "Epoch: 19400, Loss: 0.06380864232556371\n",
      "Epoch: 19500, Loss: 0.06362441256304362\n",
      "Epoch: 19600, Loss: 0.06344171048355082\n",
      "Epoch: 19700, Loss: 0.06326051537104538\n",
      "Epoch: 19800, Loss: 0.06308080689775875\n",
      "Epoch: 19900, Loss: 0.06290256511492763\n"
     ]
    }
   ],
   "source": [
    "nn.train(training_inputs, training_outputs, 20000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01795721]\n",
      " [0.9863606 ]\n",
      " [0.98685253]\n",
      " [0.01748453]]\n"
     ]
    }
   ],
   "source": [
    "print(nn.y_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
